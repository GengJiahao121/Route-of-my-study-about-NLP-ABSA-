# Route of my study for NLP(ABSA)
用来记录学习研究方向相关的学习资料，路线......

## Word2vec
- 链接：
  
  [word2vec前世今生](https://www.cnblogs.com/iloveai/p/word2vec.html), 
  
  [一文详解Softmax函数](https://zhuanlan.zhihu.com/p/105722023), 
  
  [详解softmax函数以及相关求导过程忆臻](https://zhuanlan.zhihu.com/p/25723112), 
  
  [交叉熵损失函数原理详解](https://blog.csdn.net/b1055077005/article/details/100152102)

  反向传播算法（最好手推）
  
- ppt: 

  ```ppt/2021.11.14~21 Word2Vector Softmax 交叉熵函数.pptx```

## 各种神经网络
  包括感知机, rnn, cnn , lstm, gru等经典的神经网络，都自己看一遍，打牢基础
  
- 链接（看一个链接中的就可）：
  
  [零基础入门深度学习](https://zybuluo.com/hanbingtao/note/541458),
  
  [弗雷的小屋](https://freyr-wings.github.io/2018/03/24/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN%E5%8F%8A%E5%85%B6%E5%8F%98%E7%A7%8D/),

## 注意力机制
## sequence2sequence模型
## Transformer
- Transformer论文：
  
  ```paper/Attention Is All You Need.pdf```

- Transformer原理讲解，代码讲解：

  [Transformer从零详细解读(可能是你见过最通俗易懂的讲解)](https://www.bilibili.com/video/BV1Di4y1c7Zm/?spm_id_from=333.999.0.0&vd_source=afc0b67a7b6a49641c92c82a78e417a7)

  [Transformer代码(源码Pytorch版本)从零解读(Pytorch版本）](https://www.bilibili.com/video/BV1dR4y1E7aL/?spm_id_from=333.999.0.0&vd_source=afc0b67a7b6a49641c92c82a78e417a7)

## Bert
- 链接：这个链接里面有包括注意力机制、sequence2sequence、Transformer、Bert的讲解
  
  [一文读懂BERT(原理篇)](https://blog.csdn.net/jiaowoshouzi/article/details/89073944/),

  [Bert原理讲解原创版本](https://jalammar.github.io/illustrated-transformer/),

- Bert论文：
  
  ```paper/BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf```

- Bert原理视频讲解、代码讲解：

  [BERT从零详细解读，看不懂来打我](https://www.bilibili.com/video/BV1Ey4y1874y/?spm_id_from=333.999.0.0&vd_source=afc0b67a7b6a49641c92c82a78e417a7)

  [BERT代码(源码)从零解读【Pytorch-手把手教你从零实现一个BERT源码模型】](https://www.bilibili.com/video/BV1Kb4y187G6/?spm_id_from=333.999.0.0&vd_source=afc0b67a7b6a49641c92c82a78e417a7)

  











